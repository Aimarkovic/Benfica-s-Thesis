{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5d748d",
   "metadata": {},
   "source": [
    "## Loading the data and making the association between the name of the Txt file and the position of the sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc7dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory Setup \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(f\"Directory not found: {data_dir}\")\n",
    "\n",
    "# Master Mapping: sensor position → list of device IDs from 3 sets \n",
    "position_to_ids = {\n",
    "    \"head\":      [\"00B417C1\", \"00B4351F\", \"10B41434\"],\n",
    "    \"torso\":     [\"00B417C7\", \"00B43524\", \"10B41435\"],\n",
    "    \"humerus_r\": [\"00B417CE\", \"00E13528\", \"10B41436\"],\n",
    "    \"humerus_l\": [\"00B417EB\", \"00B43529\", \"10B41437\"],\n",
    "    \"ulna_r\":    [\"00B41841\", \"00B4352A\", \"10B41438\"],\n",
    "    \"ulna_l\":    [\"00B4184F\", \"00B4352B\", \"10B4145B\"],\n",
    "    \"hand_r\":    [\"00B418AE\", \"00B4352C\", \"10B4145C\"],\n",
    "    \"hand_l\":    [\"00B44027\", \"00B43536\", \"10B41461\"],\n",
    "    \"pelvis\":    [\"00B45B76\", \"00B43537\", \"10B41462\"],\n",
    "    \"femur_r\":   [\"00B45B77\", \"00B43538\", \"10B41464\"],\n",
    "    \"femur_l\":   [\"00B45B79\", \"00B4353F\", \"10B4147D\"],\n",
    "    \"tibia_r\":   [\"00B45B7A\", \"00B43542\", \"10B4147E\"],\n",
    "    \"tibia_l\":   [\"00B45B7C\", \"00B43543\", \"10B41480\"],\n",
    "    \"calcn_r\":   [\"00B45B83\", \"00B43547\", \"10B41481\"],\n",
    "    \"calcn_l\":   [\"00B45B88\", \"00B4359C\", \"10B41487\"]\n",
    "}\n",
    "\n",
    "# Build device ID → sensor position map \n",
    "sensor_map = {}\n",
    "for position, ids in position_to_ids.items():\n",
    "    for device_id in ids:\n",
    "        sensor_map[device_id] = position\n",
    "\n",
    "# Find All TXT Files \n",
    "txt_files = [f for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(\"No TXT files found in the directory.\")\n",
    "\n",
    "print(f\"Found {len(txt_files)} TXT files: {txt_files}\\n\")\n",
    "\n",
    "# Loop Over Files \n",
    "for filename in txt_files:\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Attempt to identify the sensor based on file name\n",
    "    matched_key = next((key for key in sensor_map if key in filename), None)\n",
    "    if matched_key is None:\n",
    "        print(f\" Could not match device ID in file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    sensor_name = sensor_map[matched_key]\n",
    "    print(f\" Processing file: {filename} as '{sensor_name}'\")\n",
    "\n",
    "    #  Preview the file \n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print(f\"First 3 lines from {sensor_name}:\")\n",
    "    for i, line in enumerate(lines[:3]):\n",
    "        print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66736f54",
   "metadata": {},
   "source": [
    "## Make the file accessible for work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names based on the IMU data structure \n",
    "columns = [\"PacketCounter\", \"Roll\", \"Pitch\", \"Yaw\", \"AccX\", \"AccY\", \"AccZ\", \"GyrX\", \"GyrY\", \"GyrZ\", \"MagX\", \"MagY\", \"MagZ\"]\n",
    "\n",
    "# Master mapping: sensor position → list of all device IDs \n",
    "position_to_ids = {\n",
    "    \"head\":      [\"00B417C1\", \"00B4351F\", \"10B41434\"],\n",
    "    \"torso\":     [\"00B417C7\", \"00B43524\", \"10B41435\"],\n",
    "    \"humerus_r\": [\"00B417CE\", \"00E13528\", \"10B41436\"],\n",
    "    \"humerus_l\": [\"00B417EB\", \"00B43529\", \"10B41437\"],\n",
    "    \"ulna_r\":    [\"00B41841\", \"00B4352A\", \"10B41438\"],\n",
    "    \"ulna_l\":    [\"00B4184F\", \"00B4352B\", \"10B4145B\"],\n",
    "    \"hand_r\":    [\"00B418AE\", \"00B4352C\", \"10B4145C\"],\n",
    "    \"hand_l\":    [\"00B44027\", \"00B43536\", \"10B41461\"],\n",
    "    \"pelvis\":    [\"00B45B76\", \"00B43537\", \"10B41462\"],\n",
    "    \"femur_r\":   [\"00B45B77\", \"00B43538\", \"10B41464\"],\n",
    "    \"femur_l\":   [\"00B45B79\", \"00B4353F\", \"10B4147D\"],\n",
    "    \"tibia_r\":   [\"00B45B7A\", \"00B43542\", \"10B4147E\"],\n",
    "    \"tibia_l\":   [\"00B45B7C\", \"00B43543\", \"10B41480\"],\n",
    "    \"calcn_r\":   [\"00B45B83\", \"00B43547\", \"10B41481\"],\n",
    "    \"calcn_l\":   [\"00B45B88\", \"00B4359C\", \"10B41487\"]\n",
    "}\n",
    "\n",
    "# Flatten ID → position map\n",
    "sensor_map = {device_id: position for position, ids in position_to_ids.items() for device_id in ids}\n",
    "\n",
    "# Loop through and clean all TXT files \n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "\n",
    "    # Match IMU identifier to a body part\n",
    "    matched_key = next((key for key in sensor_map if key in file), None)\n",
    "    sensor_name = sensor_map.get(matched_key, \"unknown\")\n",
    "\n",
    "    # Load the file\n",
    "    df = pd.read_csv(file_path, delim_whitespace=True, skiprows=7, header=None, names=columns)\n",
    "\n",
    "    print(f\"\\n Loaded data for sensor: {sensor_name}\")\n",
    "    print(df.head(2))\n",
    "\n",
    "    # Save a cleaned CSV with a descriptive name\n",
    "    output_file = os.path.join(data_dir, f\"cleaned_data_{sensor_name}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\" Saved cleaned file: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05be8c",
   "metadata": {},
   "source": [
    "## Converting to the right units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define your cleaned files directory again\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "cleaned_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Function to convert units\n",
    "def convert_to_correct_units(df):\n",
    "    \"\"\"Convert gyroscope data from deg/s to rad/s and magnetometer data from Gauss to Teslas.\"\"\"\n",
    "    gyro_columns = [\"GyrX\", \"GyrY\", \"GyrZ\"]\n",
    "    mag_columns = [\"MagX\", \"MagY\", \"MagZ\"]\n",
    "    df[gyro_columns] = df[gyro_columns] * (np.pi / 180)  # Convert deg/s to rad/s\n",
    "    df[mag_columns] = df[mag_columns] * 1e-4  # Convert Gauss to Teslas\n",
    "    # Accelerometer is already in m/s², no changes needed\n",
    "    return df\n",
    "\n",
    "# Process each cleaned CSV file\n",
    "for file in cleaned_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Apply conversion\n",
    "    df_converted = convert_to_correct_units(df)\n",
    "\n",
    "    # Define output filename\n",
    "    sensor_name = file.replace(\"cleaned_data_\", \"\").replace(\".csv\", \"\")\n",
    "    converted_filename = f\"cleaned_data_converted_{sensor_name}.csv\"\n",
    "    converted_filepath = os.path.join(data_dir, converted_filename)\n",
    "\n",
    "    # Save converted file\n",
    "    df_converted.to_csv(converted_filepath, index=False)\n",
    "    print(f\"Converted and saved: {converted_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a08b6",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470e4a6",
   "metadata": {},
   "source": [
    "# GYROSCOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247f164",
   "metadata": {},
   "source": [
    "## Recognizing which type of bias drift we have in the gyroscope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712fe92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define your directory and find converted files \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Process each IMU file \n",
    "for file in converted_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Plot all gyroscope axes together \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"GyrX\"], label=\"GyrX\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrY\"], label=\"GyrY\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrZ\"], label=\"GyrZ\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Gyroscope (rad/s)\")\n",
    "    plt.title(f\"Raw Gyroscope Data - {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot each gyroscope axis separately \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "    axes[0].plot(df[\"GyrX\"], label=\"GyrX\", color=\"blue\", alpha=0.8)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[0].set_ylabel(\"GyrX (rad/s)\")\n",
    "    axes[0].set_title(f\"Gyroscope X - {sensor_name}\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(df[\"GyrY\"], label=\"GyrY\", color=\"green\", alpha=0.8)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1].set_ylabel(\"GyrY (rad/s)\")\n",
    "    axes[1].set_title(f\"Gyroscope Y - {sensor_name}\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(df[\"GyrZ\"], label=\"GyrZ\", color=\"red\", alpha=0.8)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"GyrZ (rad/s)\")\n",
    "    axes[2].set_title(f\"Gyroscope Z - {sensor_name}\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4179954",
   "metadata": {},
   "source": [
    "## Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import pywt\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Parameters \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "fs = 40  # Sampling frequency\n",
    "\n",
    "# Processing Functions \n",
    "\n",
    "def band_pass_filter(data, low_cutoff=0.1, high_cutoff=5, fs=40, order=2):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = low_cutoff / nyquist\n",
    "    high = high_cutoff / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band', analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def wavelet_denoise(data, wavelet=\"db4\", level=2):\n",
    "    coeffs = pywt.wavedec(data, wavelet, mode=\"per\")\n",
    "    sigma = np.median(np.abs(coeffs[-level])) / 0.6745\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(data)))\n",
    "    coeffs[1:] = [pywt.threshold(c, threshold, mode=\"soft\") for c in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, wavelet, mode=\"per\")[:len(data)]\n",
    "\n",
    "def kalman_filter(data):\n",
    "    kf = KalmanFilter(\n",
    "        transition_matrices=[1],\n",
    "        observation_matrices=[1],\n",
    "        initial_state_mean=0,\n",
    "        initial_state_covariance=1e-2,\n",
    "        transition_covariance=1e-3,\n",
    "        observation_covariance=1e-2\n",
    "    )\n",
    "    filtered_state_means, _ = kf.smooth(data)\n",
    "    return filtered_state_means.flatten()\n",
    "\n",
    "# Process each IMU file \n",
    "for file in converted_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #Define gyroscope columns\n",
    "    gyro_columns = [\"GyrX\", \"GyrY\", \"GyrZ\"]\n",
    "\n",
    "    # Apply Processing Pipeline\n",
    "    for axis in gyro_columns:\n",
    "        df[axis + \"_filtered\"] = band_pass_filter(df[axis], fs=fs)\n",
    "        df[axis + \"_denoised\"] = wavelet_denoise(df[axis + \"_filtered\"])\n",
    "        df[axis + \"_kalman\"] = kalman_filter(df[axis + \"_denoised\"])\n",
    "\n",
    "    # Save processed file\n",
    "    output_file = os.path.join(data_dir, f\"final_corrected_gyroscope_data_{sensor_name}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed gyroscope data saved: {output_file}\")\n",
    "\n",
    "    # Plot Results\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    colors = {\"GyrX\": \"blue\", \"GyrY\": \"green\", \"GyrZ\": \"red\"}\n",
    "\n",
    "    for i, axis in enumerate(gyro_columns):\n",
    "        corrected_axis = axis + \"_kalman\"\n",
    "        axes[i].plot(df.index, df[axis], label=f\"Raw {axis}\", color=colors[axis], alpha=0.5)\n",
    "        axes[i].plot(df.index, df[corrected_axis], label=f\"Corrected {axis}\", linestyle=\"dashed\", color=\"black\")\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[i].set_xlabel(\"Time (samples)\")\n",
    "        axes[i].set_ylabel(f\"{axis} (rad/s)\")\n",
    "        axes[i].set_title(f\"Gyroscope Correction - {axis} - {sensor_name}\")\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9883256",
   "metadata": {},
   "source": [
    "## Plot processed gyroscope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9fbe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup Directory \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "gyro_files = [f for f in os.listdir(data_dir) if f.startswith(\"final_corrected_gyroscope_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Plot Processed Gyroscope Data \n",
    "for file in gyro_files:\n",
    "    sensor_name = file.replace(\"final_corrected_gyroscope_data_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"GyrX_kalman\"], label=\"GyrX_kalman\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrY_kalman\"], label=\"GyrY_kalman\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrZ_kalman\"], label=\"GyrZ_kalman\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Gyroscope (rad/s)\")\n",
    "    plt.title(f\"Processed Gyroscope Data - {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa3423",
   "metadata": {},
   "source": [
    "# ACCELEROMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b773d",
   "metadata": {},
   "source": [
    "## Recognizing which type of drift we have in the Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53682059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup Directory \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "acc_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Loop Through Each Sensor's Cleaned Data \n",
    "for file in acc_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Overview Plot (All Axes) \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"AccX\"], label=\"AccX\", alpha=0.8)\n",
    "    plt.plot(df[\"AccY\"], label=\"AccY\", alpha=0.8)\n",
    "    plt.plot(df[\"AccZ\"], label=\"AccZ\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Accelerometer (m/s²)\")\n",
    "    plt.title(f\"Raw Accelerometer Data - {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Individual Axis Subplots \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    axes[0].plot(df[\"AccX\"], label=\"AccX\", color=\"blue\", alpha=0.8)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[0].set_ylabel(\"AccX (m/s²)\")\n",
    "    axes[0].set_title(f\"{sensor_name} - AccX\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(df[\"AccY\"], label=\"AccY\", color=\"green\", alpha=0.8)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1].set_ylabel(\"AccY (m/s²)\")\n",
    "    axes[1].set_title(f\"{sensor_name} - AccY\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(df[\"AccZ\"], label=\"AccZ\", color=\"red\", alpha=0.8)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"AccZ (m/s²)\")\n",
    "    axes[2].set_title(f\"{sensor_name} - AccZ\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9d22d",
   "metadata": {},
   "source": [
    "## Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97a73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, savgol_filter, medfilt, sosfilt\n",
    "\n",
    "# Parameters \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "fs = 40  # Sampling rate\n",
    "\n",
    "# Preprocessing Functions \n",
    "def median_and_lowpass_filter(sensor_data, fs, medfilt_window_length=11):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies a median filter followed by a butterworth lowpass filter. The lowpass filter is 3rd order with a cutoff\n",
    "    frequency of 20 Hz . The processing scheme is based on:\n",
    "    \"A Public Domain Dataset for Human Activity Recognition Using Smartphones\"\n",
    "    https://www.esann.org/sites/default/files/proceedings/legacy/es2013-84.pdf\n",
    "\n",
    "    :param sensor_data: a 1-D or (MxN) array, where M is the signal length in samples and\n",
    "                        N is the number of signals / channels.\n",
    "    :param fs: the sampling frequency of the acc data.\n",
    "    :param medfilt_window_length: the length of the median filter (has to be odd). Default: 11\n",
    "    :return: the filtered data\n",
    "    \"\"\"\n",
    "\n",
    "    #define the filter\n",
    "    order = 3\n",
    "    f_c = 18\n",
    "    sos = butter(order, f_c, fs=fs, output='sos')\n",
    "    \n",
    "    #copy the array\n",
    "    filtered_data = sensor_data.copy()\n",
    "    \n",
    "    #check the dimensionality of the input\n",
    "    if filtered_data.ndim > 1:\n",
    "        #cycle of the channels contained in data\n",
    "        for channel in range(filtered_data.shape[1]):\n",
    "            #get the channel and apply the median filter\n",
    "            sig = medfilt(sensor_data[:, channel], medfilt_window_length)\n",
    "            #apply butterworth filter\n",
    "            filtered_data[:, channel] = sosfilt(sos, sig)\n",
    "    else:\n",
    "        #apply median filter\n",
    "        med_filt = medfilt(sensor_data, medfilt_window_length)\n",
    "        #apply butterworth filter\n",
    "        filtered_data = sosfilt(sos, med_filt)\n",
    "        \n",
    "    return filtered_data\n",
    "\n",
    "def gravitational_filter(acc_data, fs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to filter out the gravitational component of ACC signals using a 3rd order butterworth lowpass filter with\n",
    "    a cuttoff frequency of 0.3 Hz\n",
    "    The implementation is based on:\n",
    "    \"A Public Domain Dataset for Human Activity Recognition Using Smartphones\"\n",
    "    https://www.esann.org/sites/default/files/proceedings/legacy/es2013-84.pdf\n",
    "    :param acc_data: a 1-D or (MxN) array, where where M is the signal length in samples and\n",
    "                 N is the number of signals / channels.\n",
    "    :param fs: the sampling frequency of the acc data.\n",
    "    :return: the gravitational component of each signal/channel contained in acc_data\n",
    "    \"\"\"\n",
    "    \n",
    "    #define the filter\n",
    "    order = 3\n",
    "    f_c = 0.5\n",
    "    sos = butter(order, f_c, fs=fs, output='sos')\n",
    "    \n",
    "    #copy the array\n",
    "    gravity_data = acc_data.copy()\n",
    "    \n",
    "    #check the dimensionality of the input\n",
    "    if gravity_data.ndim > 1:\n",
    "        #cycle of the channels contained in data\n",
    "        for channel in range(gravity_data.shape[1]):\n",
    "            #get the channel & apply butterworth filter\n",
    "            gravity_data[:, channel] = sosfilt(sos, acc_data[:, channel])\n",
    "    else:\n",
    "        gravity_data = sosfilt(sos, acc_data)\n",
    "        \n",
    "    return gravity_data\n",
    "\n",
    "def pre_process_inertial_data(sensor_data, is_acc=True, fs=40):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies the pre-processing pipeline of \"A Public Domain Dataset for Human Activity Recognition Using Smartphones\"\n",
    "    (https://www.esann.org/sites/default/files/proceedings/legacy/es2013-84.pdf). The pipeline consists of:\n",
    "    (1) applying a median filter\n",
    "    (2) applying a 3rd order low-pass filter with a cut-off at 18 Hz\n",
    "    (3) applying a 3rd order low-pass filter with a cut-off at 0.3 Hz to obtain gravitational component\n",
    "    (4) subtract gravitational component from ACC signal\n",
    "\n",
    "    :param sensor_data: the sensor data.\n",
    "    :param is_acc: boolean indicating whether the sensor is an accelerometer.\n",
    "    :param fs: the sampling frequency of the sensor data (in Hz).\n",
    "    :return: numpy.array containing the pre-processed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #apply median and lowpass filter\n",
    "    filtered_data = median_and_lowpass_filter(sensor_data, fs=fs)\n",
    "    #check if sensor is ACC\n",
    "    if is_acc:\n",
    "        #get the gravitacional component\n",
    "        gravitational_component = gravitational_filter(filtered_data, fs=fs)\n",
    "        #subtract the gravitacional component\n",
    "        filtered_data -= gravitational_component\n",
    "    return filtered_data\n",
    "\n",
    "def band_pass_filter(data, low_cutoff=0.1, high_cutoff=15, fs=40, order=3):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = low_cutoff / nyquist\n",
    "    high = high_cutoff / nyquist\n",
    "    b, a = butter(order, [low, high], btype=\"band\", analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def adaptive_baseline_correction(data, alpha=0.08):\n",
    "    baseline = np.zeros_like(data)\n",
    "    estimate = data[0]\n",
    "    for i in range(len(data)):\n",
    "        estimate = alpha * data[i] + (1 - alpha) * estimate\n",
    "        baseline[i] = data[i] - estimate\n",
    "    return baseline\n",
    "\n",
    "def savitzky_golay_smooth(data, window_size=5, poly_order=2):\n",
    "    return savgol_filter(data, window_size, poly_order)\n",
    "\n",
    "# Process Each Sensor File \n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for file in converted_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"Processing accelerometer for sensor: {sensor_name}\")\n",
    "\n",
    "    # Step 1: Remove Gravity \n",
    "    accel_data = df[[\"AccX\", \"AccY\", \"AccZ\"]].values\n",
    "    accel_no_gravity = pre_process_inertial_data(accel_data, is_acc=True, fs=fs)\n",
    "\n",
    "    # Step 2: Band-pass Filter \n",
    "    accel_bandpassed = np.apply_along_axis(band_pass_filter, 0, accel_no_gravity)\n",
    "\n",
    "    # Step 3: Baseline Correction \n",
    "    accel_corrected = np.apply_along_axis(adaptive_baseline_correction, 0, accel_bandpassed)\n",
    "\n",
    "    # Step 4: Smoothing \n",
    "    accel_smooth = np.apply_along_axis(savitzky_golay_smooth, 0, accel_corrected)\n",
    "\n",
    "    # Step 5: Save Processed Data \n",
    "    df[[\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\"]] = accel_smooth\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_accelerometer_data_{sensor_name}.csv\")\n",
    "    df.to_csv(processed_path, index=False)\n",
    "    print(f\" Saved processed accelerometer data: {processed_path}\")\n",
    "\n",
    "    # Step 6: Plot for Visual Inspection \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    colors = {\"AccX\": \"blue\", \"AccY\": \"green\", \"AccZ\": \"red\"}\n",
    "\n",
    "    for i, axis in enumerate([\"AccX\", \"AccY\", \"AccZ\"]):\n",
    "        corrected_axis = axis + \"_processed\"\n",
    "        axes[i].plot(df.index, df[axis], label=f\"Raw {axis}\", color=colors[axis], alpha=0.5)\n",
    "        axes[i].plot(df.index, df[corrected_axis], label=f\"Processed {axis}\", linestyle=\"dashed\", color=\"black\")\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[i].set_xlabel(\"Time (samples)\")\n",
    "        axes[i].set_ylabel(f\"{axis} (m/s²)\")\n",
    "        axes[i].set_title(f\"{sensor_name} - Raw vs Processed {axis}\")\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0e4e4",
   "metadata": {},
   "source": [
    "## Plot processed accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e89789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the directory containing processed accelerometer files\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "# Get all processed accelerometer files\n",
    "processed_files = [f for f in os.listdir(data_dir)\n",
    "                   if f.startswith(\"final_corrected_accelerometer_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Plot each file\n",
    "for file in processed_files:\n",
    "    sensor_name = file.replace(\"final_corrected_accelerometer_data_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "\n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Identify processed accelerometer columns\n",
    "    acc_columns = [col for col in df.columns if \"Acc\" in col and \"processed\" in col]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in acc_columns:\n",
    "        plt.plot(df[col], label=col, alpha=0.8)\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Accelerometer (m/s²)\")\n",
    "    plt.title(f\"Processed Accelerometer Data – {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f94e3",
   "metadata": {},
   "source": [
    "# MAGNETOMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0da81b",
   "metadata": {},
   "source": [
    "## Recognizing which type of  drift we have in the magnetometer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Directory with processed files\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "if not converted_files:\n",
    "    raise FileNotFoundError(\"No cleaned_data_converted_*.csv files found.\")\n",
    "\n",
    "for file in converted_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    sensor_name = file.split(\"converted_\")[1].replace(\".csv\", \"\")\n",
    "    \n",
    "    # Plot raw magnetometer data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"MagX\"], label=\"MagX\", alpha=0.8)\n",
    "    plt.plot(df[\"MagY\"], label=\"MagY\", alpha=0.8)\n",
    "    plt.plot(df[\"MagZ\"], label=\"MagZ\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Magnetometer (T)\")\n",
    "    plt.title(f\"Raw Magnetometer Data - Identifying Drift ({sensor_name})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Individual axis subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    axes[0].plot(df[\"MagX\"], label=\"MagX\", color=\"blue\", alpha=0.8)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[0].set_ylabel(\"Mag X (T)\")\n",
    "    axes[0].set_title(f\"{sensor_name} - MagX\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(df[\"MagY\"], label=\"MagY\", color=\"green\", alpha=0.8)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1].set_ylabel(\"Mag Y (T)\")\n",
    "    axes[1].set_title(f\"{sensor_name} - MagY\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(df[\"MagZ\"], label=\"MagZ\", color=\"red\", alpha=0.8)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"Mag Z (T)\")\n",
    "    axes[2].set_title(f\"{sensor_name} - MagZ\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7dd75",
   "metadata": {},
   "source": [
    "## Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806334f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, savgol_filter\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "# Functions for Magnetometer Processing \n",
    "def hard_iron_correction(mag_data):\n",
    "    \"\"\"Apply hard-iron correction using median instead of mean to remove constant bias.\"\"\"\n",
    "    bias = np.median(mag_data, axis=0)\n",
    "    return mag_data - bias\n",
    "\n",
    "def ellipsoid_fit(mag_data):\n",
    "    \"\"\"Estimate soft-iron correction using ellipsoid fitting.\"\"\"\n",
    "    def cost_function(params, x, y, z):\n",
    "        A, B, C, D, E, F, G, H, I, J = params\n",
    "        return (A*x**2 + B*y**2 + C*z**2 + 2*D*x*y + 2*E*x*z + 2*F*y*z +\n",
    "                2*G*x + 2*H*y + 2*I*z + J)\n",
    "    \n",
    "    X, Y, Z = mag_data[:, 0], mag_data[:, 1], mag_data[:, 2]\n",
    "    initial_guess = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, -1])\n",
    "    result = least_squares(cost_function, initial_guess, args=(X, Y, Z))\n",
    "\n",
    "    # Extract transformation matrix parameters\n",
    "    A, B, C, D, E, F, G, H, I, J = result.x\n",
    "    M_inv_raw = np.array([[A, D, E], [D, B, F], [E, F, C]])\n",
    "    \n",
    "    # Normalize matrix using Singular Value Decomposition (SVD)\n",
    "    U, S, Vt = np.linalg.svd(M_inv_raw)\n",
    "    scale_factor = np.median(S)\n",
    "    M_inv = np.linalg.inv(M_inv_raw / scale_factor)\n",
    "    \n",
    "    # Extract offset (soft-iron bias)\n",
    "    soft_iron_bias = np.array([G, H, I])\n",
    "    return M_inv, soft_iron_bias\n",
    "\n",
    "def band_pass_filter(data, low_cutoff=0.5, high_cutoff=10, fs=40, order=2):\n",
    "    \"\"\"Apply a Butterworth band-pass filter to remove both low-frequency drift and high-frequency noise.\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = low_cutoff / nyquist\n",
    "    high = high_cutoff / nyquist\n",
    "    b, a = butter(order, [low, high], btype=\"band\", analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def savitzky_golay_smooth(data, window_size=9, poly_order=2):\n",
    "    \"\"\"Apply Savitzky-Golay smoothing to reduce noise while preserving trends.\"\"\"\n",
    "    return savgol_filter(data, window_size, poly_order)\n",
    "\n",
    "# Process All Cleaned Files \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "cleaned_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "if not cleaned_files:\n",
    "    raise FileNotFoundError(\"No cleaned_data_converted_<sensor>.csv files found.\")\n",
    "\n",
    "for file in cleaned_files:\n",
    "    sensor_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(sensor_path)\n",
    "    sensor_name = file.split(\"cleaned_data_converted_\")[1].replace(\".csv\", \"\")\n",
    "    df_raw = df.copy()  # For plotting later\n",
    "\n",
    "    # Extract and process magnetometer data\n",
    "    mag_columns = [\"MagX\", \"MagY\", \"MagZ\"]\n",
    "    mag_data = df[mag_columns].values\n",
    "\n",
    "    mag_hard_iron = hard_iron_correction(mag_data)\n",
    "    M_inv, soft_iron_bias = ellipsoid_fit(mag_hard_iron)\n",
    "    mag_soft_iron = np.dot(M_inv, (mag_hard_iron - soft_iron_bias).T).T\n",
    "    mag_filtered = np.apply_along_axis(band_pass_filter, 0, mag_soft_iron)\n",
    "    mag_smooth = np.apply_along_axis(savitzky_golay_smooth, 0, mag_filtered)\n",
    "\n",
    "    # Store in DataFrame\n",
    "    df[[\"MagX_processed\", \"MagY_processed\", \"MagZ_processed\"]] = mag_smooth\n",
    "\n",
    "    # Save file\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "    df.to_csv(processed_path, index=False)\n",
    "    print(f\"Saved magnetometer data for sensor: {sensor_name}\")\n",
    "\n",
    "# Plot 4 Graphs Per Sensor \n",
    "for file in cleaned_files:\n",
    "    sensor_name = file.split(\"cleaned_data_converted_\")[1].replace(\".csv\", \"\")\n",
    "    raw_path = os.path.join(data_dir, f\"cleaned_data_converted_{sensor_name}.csv\")\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "    \n",
    "    if not os.path.exists(raw_path) or not os.path.exists(processed_path):\n",
    "        continue\n",
    "\n",
    "    df_raw = pd.read_csv(raw_path)\n",
    "    df_proc = pd.read_csv(processed_path)\n",
    "\n",
    "    # Combined MagX, MagY, MagZ Raw vs Processed \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df_raw[\"MagX\"], label=\"Raw MagX\", color=\"blue\", alpha=0.3)\n",
    "    plt.plot(df_raw[\"MagY\"], label=\"Raw MagY\", color=\"green\", alpha=0.3)\n",
    "    plt.plot(df_raw[\"MagZ\"], label=\"Raw MagZ\", color=\"red\", alpha=0.3)\n",
    "\n",
    "    plt.plot(df_proc[\"MagX_processed\"], label=\"Processed MagX\", color=\"blue\", linestyle=\"--\")\n",
    "    plt.plot(df_proc[\"MagY_processed\"], label=\"Processed MagY\", color=\"green\", linestyle=\"--\")\n",
    "    plt.plot(df_proc[\"MagZ_processed\"], label=\"Processed MagZ\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.title(f\"{sensor_name} - Magnetometer Raw vs Processed (Combined)\")\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Magnetic Field (T)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Separate Axes Plots: Raw vs Processed \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "    colors = [\"blue\", \"green\", \"red\"]\n",
    "    mag_axes = [\"MagX\", \"MagY\", \"MagZ\"]\n",
    "\n",
    "    for i, axis in enumerate(mag_axes):\n",
    "        axes[i].plot(df_raw[axis], label=f\"Raw {axis}\", color=colors[i], alpha=0.3)\n",
    "        axes[i].plot(df_proc[f\"{axis}_processed\"], label=f\"Processed {axis}\", linestyle=\"--\", color=\"black\")\n",
    "        axes[i].set_title(f\"{sensor_name} - {axis} Raw vs Processed\")\n",
    "        axes[i].set_ylabel(\"Magnetic Field (T)\")\n",
    "        axes[i].set_xlabel(\"Time (samples)\")\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7332c",
   "metadata": {},
   "source": [
    "## Plot processed magnetometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352eeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loop through each processed file \n",
    "for file in cleaned_files:\n",
    "    sensor_name = file.split(\"cleaned_data_converted_\")[1].replace(\".csv\", \"\")\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed file not found for sensor: {sensor_name}\")\n",
    "        continue\n",
    "\n",
    "    # Load the processed DataFrame\n",
    "    df = pd.read_csv(processed_path)\n",
    "\n",
    "    # Print column names to verify structure (optional debug)\n",
    "    print(f\"\\nProcessed columns for {sensor_name}:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Identify processed magnetometer columns\n",
    "    mag_columns = [col for col in df.columns if \"Mag\" in col and \"processed\" in col]\n",
    "    print(f\"Processed magnetometer columns for {sensor_name}:\", mag_columns)\n",
    "\n",
    "    # Plot processed magnetometer data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in mag_columns:\n",
    "        plt.plot(df[col], label=col, alpha=0.8)\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Magnetometer (T)\")\n",
    "    plt.title(f\"{sensor_name} - Processed Magnetometer Data\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e14754",
   "metadata": {},
   "source": [
    "## Creating the combined final data file for the sensor fusion operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define data directory\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "# Get all processed accelerometer files\n",
    "accel_files = [f for f in os.listdir(data_dir) if f.startswith(\"final_corrected_accelerometer_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for accel_file in accel_files:\n",
    "    # Extract sensor name\n",
    "    sensor_name = accel_file.replace(\"final_corrected_accelerometer_data_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    # Construct paths for all 3 processed files\n",
    "    accel_path = os.path.join(data_dir, f\"final_corrected_accelerometer_data_{sensor_name}.csv\")\n",
    "    gyro_path = os.path.join(data_dir, f\"final_corrected_gyroscope_data_{sensor_name}.csv\")\n",
    "    mag_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "\n",
    "    # Check if all 3 files exist\n",
    "    if not os.path.exists(accel_path) or not os.path.exists(gyro_path) or not os.path.exists(mag_path):\n",
    "        print(f\"Skipping {sensor_name}: One or more processed files are missing.\")\n",
    "        continue\n",
    "\n",
    "    # Load data\n",
    "    accel_df = pd.read_csv(accel_path)\n",
    "    gyro_df = pd.read_csv(gyro_path)\n",
    "    mag_df = pd.read_csv(mag_path)\n",
    "\n",
    "    # Required columns\n",
    "    accel_columns = [\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\"]\n",
    "    gyro_columns = [\"GyrX_kalman\", \"GyrY_kalman\", \"GyrZ_kalman\"]\n",
    "    mag_columns = [\"MagX_processed\", \"MagY_processed\", \"MagZ_processed\", \"PacketCounter\", \"Roll\", \"Pitch\", \"Yaw\"]\n",
    "\n",
    "    # Check column availability\n",
    "    for col in accel_columns:\n",
    "        if col not in accel_df.columns:\n",
    "            raise ValueError(f\"Missing column in {accel_file}: {col}\")\n",
    "    for col in gyro_columns:\n",
    "        if col not in gyro_df.columns:\n",
    "            raise ValueError(f\"Missing column in {gyro_file}: {col}\")\n",
    "    for col in mag_columns:\n",
    "        if col not in mag_df.columns:\n",
    "            raise ValueError(f\"Missing column in {mag_file}: {col}\")\n",
    "\n",
    "    # Concatenate the selected columns\n",
    "    combined_df = pd.concat([\n",
    "        accel_df[accel_columns].reset_index(drop=True),\n",
    "        gyro_df[gyro_columns].reset_index(drop=True),\n",
    "        mag_df[mag_columns].reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Save the combined dataset\n",
    "    combined_path = os.path.join(data_dir, f\"final_combined_sensor_data_{sensor_name}.csv\")\n",
    "    combined_df.to_csv(combined_path, index=False)\n",
    "    print(f\"Saved final combined data for sensor: {sensor_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaea6bd",
   "metadata": {},
   "source": [
    "## Estimating the Roll, Pitch and Yaw values based on the measured pre-processing sensor data through Sensor Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7636ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ahrs.filters import Mahony\n",
    "from ahrs.common.orientation import q2euler\n",
    "\n",
    "# Settings \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "fs = 40  # Sampling rate (Hz)\n",
    "dt = 1 / fs\n",
    "\n",
    "# Find All Sensor Files \n",
    "combined_files = [f for f in os.listdir(data_dir)\n",
    "                  if f.startswith(\"final_combined_sensor_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Process Each Sensor File \n",
    "for file in combined_files:\n",
    "    sensor_name = file.replace(\"final_combined_sensor_data_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"\\nProcessing {sensor_name} with Mahony filter...\")\n",
    "\n",
    "    # Required sensor columns\n",
    "    required_cols = [\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\",\n",
    "                     \"GyrX_kalman\", \"GyrY_kalman\", \"GyrZ_kalman\",\n",
    "                     \"MagX_processed\", \"MagY_processed\", \"MagZ_processed\"]\n",
    "    \n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Skipping {sensor_name}: Missing required columns.\")\n",
    "        continue\n",
    "\n",
    "    # Extract Sensor Data \n",
    "    acc = df[[\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\"]].values\n",
    "    gyr = df[[\"GyrX_kalman\", \"GyrY_kalman\", \"GyrZ_kalman\"]].values\n",
    "    mag = df[[\"MagX_processed\", \"MagY_processed\", \"MagZ_processed\"]].values\n",
    "\n",
    "    # Initialize Mahony Filter \n",
    "    mahony = Mahony(sampleperiod=dt)\n",
    "    q = np.zeros((len(df), 4))\n",
    "    q[0] = np.array([1.0, 0.0, 0.0, 0.0])  # initial quaternion\n",
    "\n",
    "    # Apply Mahony Filter \n",
    "    for i in range(1, len(df)):\n",
    "        q[i] = mahony.updateIMU(q[i-1], gyr=gyr[i], acc=acc[i])\n",
    "        # To include magnetometer I should use mahony.update(), but it isn't supported by my ahrs version\n",
    "        # Code: q[i] = mahony.update(q[i-1], gyr=gyr[i], acc=acc[i], mag=mag[i])\n",
    "\n",
    "    # Convert Quaternion to Euler Angles \n",
    "    euler_rad = np.array([q2euler(qi) for qi in q]) #ISTO INFLUENCIA NALGUMA COISA?? QUATERNION->EULER->QUATERNION?\n",
    "    euler_deg = np.rad2deg(euler_rad)\n",
    "\n",
    "    # Store Orientation Estimations \n",
    "    df[\"Roll_Estimated\"] = euler_deg[:, 0]\n",
    "    df[\"Pitch_Estimated\"] = euler_deg[:, 1]\n",
    "    df[\"Yaw_Estimated\"] = euler_deg[:, 2]\n",
    "\n",
    "    # Save Output \n",
    "    output_path = os.path.join(data_dir, f\"final_orientation_estimation_{sensor_name}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved Mahony-estimated orientation for {sensor_name} to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1f5a8",
   "metadata": {},
   "source": [
    "## Comparing the calculated orientation values in the previous step with the measured Roll, Pitch and Yaw by the IMUs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9149fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory with processed orientation files\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "# Find all orientation files\n",
    "orientation_files = [f for f in os.listdir(data_dir) if f.startswith(\"final_orientation_estimation_\") and f.endswith(\".csv\")]\n",
    "\n",
    "if not orientation_files:\n",
    "    raise FileNotFoundError(\"No orientation estimation files found.\")\n",
    "\n",
    "for file in orientation_files:\n",
    "    sensor_name = file.replace(\"final_orientation_estimation_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"\\n Plotting Measured vs Estimated Orientation for: {sensor_name}\")\n",
    "\n",
    "    # Plot Roll, Pitch, Yaw Comparison \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "    axes[0].plot(df[\"Roll\"], label=\"Measured Roll\", color='blue', alpha=0.6)\n",
    "    axes[0].plot(df[\"Roll_Estimated\"], label=\"Estimated Roll\", linestyle=\"dashed\", color='black')\n",
    "    axes[0].set_ylabel(\"Angle (°)\")\n",
    "    axes[0].set_title(f\"{sensor_name} - Roll: Measured vs Estimated\")\n",
    "    axes[0].legend()\n",
    "    axes[0].axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    axes[1].plot(df[\"Pitch\"], label=\"Measured Pitch\", color='green', alpha=0.6)\n",
    "    axes[1].plot(df[\"Pitch_Estimated\"], label=\"Estimated Pitch\", linestyle=\"dashed\", color='black')\n",
    "    axes[1].set_ylabel(\"Angle (°)\")\n",
    "    axes[1].set_title(f\"{sensor_name} - Pitch: Measured vs Estimated\")\n",
    "    axes[1].legend()\n",
    "    axes[1].axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    axes[2].plot(df[\"Yaw\"], label=\"Measured Yaw\", color='red', alpha=0.6)\n",
    "    axes[2].plot(df[\"Yaw_Estimated\"], label=\"Estimated Yaw\", linestyle=\"dashed\", color='black')\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"Angle (°)\")\n",
    "    axes[2].set_title(f\"{sensor_name} - Yaw: Measured vs Estimated\")\n",
    "    axes[2].legend()\n",
    "    axes[2].axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1ad7d",
   "metadata": {},
   "source": [
    "## Convert Euler angle orientation-based data to quaternion units for OpenSim interpretation &  Creating the orientation files for OpenSim Inverse Kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200109e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# Settings \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "sampling_rate = 40  # Hz\n",
    "dt = 1 / sampling_rate\n",
    "\n",
    "# Find Euler-based files \n",
    "euler_files = [f for f in os.listdir(data_dir)\n",
    "               if f.startswith(\"final_orientation_estimation_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for file in euler_files:\n",
    "    segment = file.replace(\"final_orientation_estimation_\", \"\").replace(\".csv\", \"\")\n",
    "    path = os.path.join(data_dir, file)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Ensure time column\n",
    "    if \"PacketCounter\" in df.columns:\n",
    "        df[\"time\"] = df[\"PacketCounter\"] / sampling_rate\n",
    "        df[\"time\"] -= df[\"time\"].iloc[0]\n",
    "    elif \"time\" not in df.columns:\n",
    "        df[\"time\"] = np.arange(len(df)) * dt\n",
    "\n",
    "    # Convert Euler degrees → radians\n",
    "    euler_rad = np.deg2rad(df[[\"Roll_Estimated\", \"Pitch_Estimated\", \"Yaw_Estimated\"]].values)\n",
    "\n",
    "    # Convert to quaternions (XYZ order)\n",
    "    rotations = R.from_euler(\"xyz\", euler_rad)\n",
    "    quats = rotations.as_quat()  # (x, y, z, w)\n",
    "\n",
    "    # Format as OpenSim-compatible .sto\n",
    "    quat_df = pd.DataFrame({\n",
    "        \"time\": df[\"time\"],\n",
    "        f\"{segment}_IMU\": [\",\".join(map(str, q)) for q in quats]\n",
    "    })\n",
    "\n",
    "    # Save .sto file\n",
    "    output_file = os.path.join(data_dir, f\"{segment}_IMU_orientation_quat.sto\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"DataRate=40\\n\")\n",
    "        f.write(\"DataType=Quaternion\\n\")\n",
    "        f.write(\"version=3\\n\")\n",
    "        f.write(\"OpenSimVersion=4.4\\n\")\n",
    "        f.write(\"endheader\\n\")\n",
    "        quat_df.to_csv(f, sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\"Saved quaternion .sto for {segment} → {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba3279",
   "metadata": {},
   "source": [
    "## Creating the movement orientation file for OpenSim Inverse Kinematics (Quaternion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Configuration \n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "output_file = os.path.join(data_dir, \"movement.sto\")\n",
    "\n",
    "# List all IMU orientation files\n",
    "sto_files = sorted([\n",
    "    f for f in os.listdir(data_dir)\n",
    "    if f.endswith(\"_IMU_orientation_quat.sto\")\n",
    "])\n",
    "\n",
    "combined_data = None\n",
    "time_column = None\n",
    "header_info = None\n",
    "\n",
    "for file in sto_files:\n",
    "    path = os.path.join(data_dir, file)\n",
    "\n",
    "    # Read file and separate header from data\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_end = next(i for i, line in enumerate(lines) if line.strip() == \"endheader\")\n",
    "    header = lines[:header_end+1]\n",
    "    data_block = \"\".join(lines[header_end+1:])\n",
    "\n",
    "    # Store the first header for reuse\n",
    "    if header_info is None:\n",
    "        header_info = header\n",
    "\n",
    "    # Load data, rename first column to 'time'\n",
    "    df = pd.read_csv(StringIO(data_block), sep=\"\\t\")\n",
    "    df.columns = [\"time\"] + df.columns[1:].tolist()  # force first column name to 'time'\n",
    "\n",
    "    # Rename *_IMU to *_imu immediately\n",
    "    df.rename(columns={\n",
    "        col: col.lower() if col.endswith(\"_IMU\") else col\n",
    "        for col in df.columns\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Store time column from first file\n",
    "    if time_column is None:\n",
    "        time_column = df[\"time\"]\n",
    "\n",
    "    # Drop time column to prepare for concat\n",
    "    df = df.drop(columns=[\"time\"])\n",
    "\n",
    "    if combined_data is None:\n",
    "        combined_data = df\n",
    "    else:\n",
    "        combined_data = pd.concat([combined_data, df], axis=1)\n",
    "\n",
    "# Insert time column at the front\n",
    "combined_data.insert(0, \"time\", time_column)\n",
    "\n",
    "# Save final file \n",
    "with open(output_file, \"w\") as f:\n",
    "    f.writelines(header_info)\n",
    "    f.write(\"\\n\")\n",
    "    combined_data.to_csv(f, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\" File saved without time column:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb76dd",
   "metadata": {},
   "source": [
    "## Creating the orientation file at placement pose for IMU placement in OpenSim (Quaternions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Configuration ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "pattern = \"_IMU_orientation_quat.sto\"\n",
    "output_file = os.path.join(data_dir, \"placement_pose.sto\")\n",
    "\n",
    "# === Collect all .sto files ===\n",
    "sto_files = [f for f in os.listdir(data_dir) if f.endswith(pattern)]\n",
    "\n",
    "placement_data = None\n",
    "header_info = []\n",
    "\n",
    "for file in sto_files:\n",
    "    path = os.path.join(data_dir, file)\n",
    "\n",
    "    # Read header and data\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_end = next(i for i, line in enumerate(lines) if line.strip() == \"endheader\")\n",
    "    header = lines[:header_end+1]\n",
    "    data_block = \"\".join(lines[header_end+1:])\n",
    "\n",
    "    # Store one shared header for output\n",
    "    if not header_info:\n",
    "        header_info = header\n",
    "\n",
    "    # Read the data section\n",
    "    from io import StringIO\n",
    "    df = pd.read_csv(StringIO(data_block), sep=\"\\t\")\n",
    "\n",
    "    # Take only the first row\n",
    "    first_row = df.iloc[[0]].copy()\n",
    "\n",
    "    # Merge all first rows on 'time'\n",
    "    if placement_data is None:\n",
    "        placement_data = first_row\n",
    "    else:\n",
    "        placement_data = pd.merge(\n",
    "            placement_data,\n",
    "            first_row.drop(columns=[\"time\"]),\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "\n",
    "#  Rename columns *_IMU to columns *_imu for OpenSim interpretation\n",
    "new_columns = {\n",
    "    col: col.lower() if col.endswith(\"_IMU\") else col\n",
    "    for col in placement_data.columns\n",
    "}\n",
    "placement_data.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "#  Save combined placement pose file \n",
    "with open(output_file, \"w\") as f:\n",
    "    f.writelines(header_info)\n",
    "    f.write(\"\\n\")\n",
    "    placement_data.to_csv(f, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Full IMU placement pose file saved to:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ee6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b558e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b4c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d07719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2e772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbcc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c199740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597932e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
