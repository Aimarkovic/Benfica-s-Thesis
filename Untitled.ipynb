{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5d748d",
   "metadata": {},
   "source": [
    "## Loading the data and making the association between the name of the Txt file and the position of the sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Directory Setup ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(f\"Directory not found: {data_dir}\")\n",
    "\n",
    "# === Map of Device ID to Sensor Position ===\n",
    "sensor_map = {\n",
    "    \"00B417C1\": \"head\",\n",
    "    \"00B417C7\": \"torso\",\n",
    "    \"00B417CE\": \"humerus_r\",\n",
    "    \"00B417EB\": \"humerus_l\",\n",
    "    \"00B41841\": \"ulna_r\",\n",
    "    \"00B4184F\": \"ulna_l\",\n",
    "    \"00B418AE\": \"hand_r\",\n",
    "    \"00B44027\": \"hand_l\",\n",
    "    \"00B45B76\": \"pelvis\",\n",
    "    \"00B45B77\": \"femur_r\",\n",
    "    \"00B45B79\": \"femur_l\",\n",
    "    \"00B45B7A\": \"tibia_r\",\n",
    "    \"00B45B7C\": \"tibia_l\",\n",
    "    \"00B45B83\": \"calcn_r\",\n",
    "    \"00B45B88\": \"calcn_l\"\n",
    "}\n",
    "\n",
    "# === Find All TXT Files ===\n",
    "txt_files = [f for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(\"No TXT files found in the directory.\")\n",
    "\n",
    "print(f\"Found {len(txt_files)} TXT files: {txt_files}\\n\")\n",
    "\n",
    "# === Loop Over Files ===\n",
    "for filename in txt_files:\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Attempt to identify the sensor based on file name\n",
    "    matched_key = next((key for key in sensor_map if key in filename), None)\n",
    "    if matched_key is None:\n",
    "        print(f\" Could not match device ID in file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    sensor_name = sensor_map[matched_key]\n",
    "    print(f\" Processing file: {filename} as '{sensor_name}'\")\n",
    "\n",
    "    # === Preview the file \n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print(f\"First 3 lines from {sensor_name}:\")\n",
    "    for i, line in enumerate(lines[:3]):\n",
    "        print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66736f54",
   "metadata": {},
   "source": [
    "## Make the file accessible for work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define column names based on the IMU data structure ===\n",
    "columns = [\"PacketCounter\", \"Roll\", \"Pitch\", \"Yaw\", \"AccX\", \"AccY\", \"AccZ\", \"GyrX\", \"GyrY\", \"GyrZ\", \"MagX\", \"MagY\", \"MagZ\"]\n",
    "\n",
    "# === Map IMU identifiers to body segment names ===\n",
    "sensor_map = {\n",
    "    \"00B417C1\": \"head\",\n",
    "    \"00B417C7\": \"torso\",\n",
    "    \"00B417CE\": \"humerus_r\",\n",
    "    \"00B417EB\": \"humerus_l\",\n",
    "    \"00B41841\": \"ulna_r\",\n",
    "    \"00B4184F\": \"ulna_l\",\n",
    "    \"00B418AE\": \"hand_r\",\n",
    "    \"00B44027\": \"hand_l\",\n",
    "    \"00B45B76\": \"pelvis\",\n",
    "    \"00B45B77\": \"femur_r\",\n",
    "    \"00B45B79\": \"femur_l\",\n",
    "    \"00B45B7A\": \"tibia_r\",\n",
    "    \"00B45B7C\": \"tibia_l\",\n",
    "    \"00B45B83\": \"calcn_r\",\n",
    "    \"00B45B88\": \"calcn_l\"\n",
    "}\n",
    "\n",
    "# === Loop through and clean all TXT files ===\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "\n",
    "    # Match IMU identifier to a body part\n",
    "    matched_key = next((key for key in sensor_map if key in file), None)\n",
    "    sensor_name = sensor_map.get(matched_key, \"unknown\")\n",
    "\n",
    "    # Load the file\n",
    "    df = pd.read_csv(file_path, delim_whitespace=True, skiprows=7, header=None, names=columns)\n",
    "\n",
    "    print(f\"\\n Loaded data for sensor: {sensor_name}\")\n",
    "    print(df.head(2))\n",
    "\n",
    "    # Save a cleaned CSV with a descriptive name\n",
    "    output_file = os.path.join(data_dir, f\"cleaned_data_{sensor_name}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\" Saved cleaned file: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05be8c",
   "metadata": {},
   "source": [
    "## Converting to the right units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define your cleaned files directory again\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "cleaned_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Function to convert units\n",
    "def convert_to_correct_units(df):\n",
    "    \"\"\"Convert gyroscope data from deg/s to rad/s and magnetometer data from Gauss to Teslas.\"\"\"\n",
    "    gyro_columns = [\"GyrX\", \"GyrY\", \"GyrZ\"]\n",
    "    mag_columns = [\"MagX\", \"MagY\", \"MagZ\"]\n",
    "    df[gyro_columns] = df[gyro_columns] * (np.pi / 180)  # Convert deg/s to rad/s\n",
    "    df[mag_columns] = df[mag_columns] * 1e-4  # Convert Gauss to Teslas\n",
    "    # Accelerometer is already in m/sÂ², no changes needed\n",
    "    return df\n",
    "\n",
    "# Process each cleaned CSV file\n",
    "for file in cleaned_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Apply conversion\n",
    "    df_converted = convert_to_correct_units(df)\n",
    "\n",
    "    # Define output filename\n",
    "    sensor_name = file.replace(\"cleaned_data_\", \"\").replace(\".csv\", \"\")\n",
    "    converted_filename = f\"cleaned_data_converted_{sensor_name}.csv\"\n",
    "    converted_filepath = os.path.join(data_dir, converted_filename)\n",
    "\n",
    "    # Save converted file\n",
    "    df_converted.to_csv(converted_filepath, index=False)\n",
    "    print(f\"Converted and saved: {converted_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a08b6",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470e4a6",
   "metadata": {},
   "source": [
    "# GYROSCOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247f164",
   "metadata": {},
   "source": [
    "## Recognizing which type of bias drift we have in the gyroscope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712fe92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Define your directory and find converted files ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# === Process each IMU file ===\n",
    "for file in converted_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # === Plot all gyroscope axes together ===\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"GyrX\"], label=\"GyrX\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrY\"], label=\"GyrY\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrZ\"], label=\"GyrZ\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Gyroscope (rad/s)\")\n",
    "    plt.title(f\"Raw Gyroscope Data - {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Plot each gyroscope axis separately ===\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "    axes[0].plot(df[\"GyrX\"], label=\"GyrX\", color=\"blue\", alpha=0.8)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[0].set_ylabel(\"GyrX (rad/s)\")\n",
    "    axes[0].set_title(f\"Gyroscope X - {sensor_name}\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(df[\"GyrY\"], label=\"GyrY\", color=\"green\", alpha=0.8)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1].set_ylabel(\"GyrY (rad/s)\")\n",
    "    axes[1].set_title(f\"Gyroscope Y - {sensor_name}\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(df[\"GyrZ\"], label=\"GyrZ\", color=\"red\", alpha=0.8)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"GyrZ (rad/s)\")\n",
    "    axes[2].set_title(f\"Gyroscope Z - {sensor_name}\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4179954",
   "metadata": {},
   "source": [
    "## Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import pywt\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# === Parameters ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "fs = 40  # Sampling frequency\n",
    "\n",
    "# === Processing Functions ===\n",
    "\n",
    "def band_pass_filter(data, low_cutoff=0.1, high_cutoff=5, fs=40, order=2):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = low_cutoff / nyquist\n",
    "    high = high_cutoff / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band', analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def wavelet_denoise(data, wavelet=\"db4\", level=2):\n",
    "    coeffs = pywt.wavedec(data, wavelet, mode=\"per\")\n",
    "    sigma = np.median(np.abs(coeffs[-level])) / 0.6745\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(data)))\n",
    "    coeffs[1:] = [pywt.threshold(c, threshold, mode=\"soft\") for c in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, wavelet, mode=\"per\")[:len(data)]\n",
    "\n",
    "def kalman_filter(data):\n",
    "    kf = KalmanFilter(\n",
    "        transition_matrices=[1],\n",
    "        observation_matrices=[1],\n",
    "        initial_state_mean=0,\n",
    "        initial_state_covariance=1e-2,\n",
    "        transition_covariance=1e-3,\n",
    "        observation_covariance=1e-2\n",
    "    )\n",
    "    filtered_state_means, _ = kf.smooth(data)\n",
    "    return filtered_state_means.flatten()\n",
    "\n",
    "# === Process each IMU file ===\n",
    "for file in converted_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #Define gyroscope columns\n",
    "    gyro_columns = [\"GyrX\", \"GyrY\", \"GyrZ\"]\n",
    "\n",
    "    # Apply Processing Pipeline\n",
    "    for axis in gyro_columns:\n",
    "        df[axis + \"_filtered\"] = band_pass_filter(df[axis], fs=fs)\n",
    "        df[axis + \"_denoised\"] = wavelet_denoise(df[axis + \"_filtered\"])\n",
    "        df[axis + \"_kalman\"] = kalman_filter(df[axis + \"_denoised\"])\n",
    "\n",
    "    # Save processed file\n",
    "    output_file = os.path.join(data_dir, f\"final_corrected_gyroscope_data_{sensor_name}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed gyroscope data saved: {output_file}\")\n",
    "\n",
    "    # Plot Results\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    colors = {\"GyrX\": \"blue\", \"GyrY\": \"green\", \"GyrZ\": \"red\"}\n",
    "\n",
    "    for i, axis in enumerate(gyro_columns):\n",
    "        corrected_axis = axis + \"_kalman\"\n",
    "        axes[i].plot(df.index, df[axis], label=f\"Raw {axis}\", color=colors[axis], alpha=0.5)\n",
    "        axes[i].plot(df.index, df[corrected_axis], label=f\"Corrected {axis}\", linestyle=\"dashed\", color=\"black\")\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[i].set_xlabel(\"Time (samples)\")\n",
    "        axes[i].set_ylabel(f\"{axis} (rad/s)\")\n",
    "        axes[i].set_title(f\"Gyroscope Correction - {axis} - {sensor_name}\")\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9883256",
   "metadata": {},
   "source": [
    "## Plot processed gyroscope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9fbe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Setup Directory ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "gyro_files = [f for f in os.listdir(data_dir) if f.startswith(\"final_corrected_gyroscope_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# === Plot Processed Gyroscope Data ===\n",
    "for file in gyro_files:\n",
    "    sensor_name = file.replace(\"final_corrected_gyroscope_data_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"GyrX_kalman\"], label=\"GyrX_kalman\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrY_kalman\"], label=\"GyrY_kalman\", alpha=0.8)\n",
    "    plt.plot(df[\"GyrZ_kalman\"], label=\"GyrZ_kalman\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Gyroscope (rad/s)\")\n",
    "    plt.title(f\"Processed Gyroscope Data - {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa3423",
   "metadata": {},
   "source": [
    "# ACCELEROMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b773d",
   "metadata": {},
   "source": [
    "## Recognizing which type of drift we have in the Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53682059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Setup Directory ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "acc_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# === Loop Through Each Sensor's Cleaned Data ===\n",
    "for file in acc_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # --- Overview Plot (All Axes) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"AccX\"], label=\"AccX\", alpha=0.8)\n",
    "    plt.plot(df[\"AccY\"], label=\"AccY\", alpha=0.8)\n",
    "    plt.plot(df[\"AccZ\"], label=\"AccZ\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Accelerometer (m/sÂ²)\")\n",
    "    plt.title(f\"Raw Accelerometer Data - {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Individual Axis Subplots ---\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    axes[0].plot(df[\"AccX\"], label=\"AccX\", color=\"blue\", alpha=0.8)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[0].set_ylabel(\"AccX (m/sÂ²)\")\n",
    "    axes[0].set_title(f\"{sensor_name} - AccX\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(df[\"AccY\"], label=\"AccY\", color=\"green\", alpha=0.8)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1].set_ylabel(\"AccY (m/sÂ²)\")\n",
    "    axes[1].set_title(f\"{sensor_name} - AccY\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(df[\"AccZ\"], label=\"AccZ\", color=\"red\", alpha=0.8)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"AccZ (m/sÂ²)\")\n",
    "    axes[2].set_title(f\"{sensor_name} - AccZ\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9d22d",
   "metadata": {},
   "source": [
    "## Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97a73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, savgol_filter, medfilt, sosfilt\n",
    "\n",
    "# === Parameters ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "fs = 40  # Sampling rate\n",
    "\n",
    "# === Preprocessing Functions ===\n",
    "def median_and_lowpass_filter(sensor_data, fs, medfilt_window_length=11):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies a median filter followed by a butterworth lowpass filter. The lowpass filter is 3rd order with a cutoff\n",
    "    frequency of 20 Hz . The processing scheme is based on:\n",
    "    \"A Public Domain Dataset for Human Activity Recognition Using Smartphones\"\n",
    "    https://www.esann.org/sites/default/files/proceedings/legacy/es2013-84.pdf\n",
    "\n",
    "    :param sensor_data: a 1-D or (MxN) array, where M is the signal length in samples and\n",
    "                        N is the number of signals / channels.\n",
    "    :param fs: the sampling frequency of the acc data.\n",
    "    :param medfilt_window_length: the length of the median filter (has to be odd). Default: 11\n",
    "    :return: the filtered data\n",
    "    \"\"\"\n",
    "\n",
    "    #define the filter\n",
    "    order = 3\n",
    "    f_c = 18\n",
    "    sos = butter(order, f_c, fs=fs, output='sos')\n",
    "    \n",
    "    #copy the array\n",
    "    filtered_data = sensor_data.copy()\n",
    "    \n",
    "    #check the dimensionality of the input\n",
    "    if filtered_data.ndim > 1:\n",
    "        #cycle of the channels contained in data\n",
    "        for channel in range(filtered_data.shape[1]):\n",
    "            #get the channel and apply the median filter\n",
    "            sig = medfilt(sensor_data[:, channel], medfilt_window_length)\n",
    "            #apply butterworth filter\n",
    "            filtered_data[:, channel] = sosfilt(sos, sig)\n",
    "    else:\n",
    "        #apply median filter\n",
    "        med_filt = medfilt(sensor_data, medfilt_window_length)\n",
    "        #apply butterworth filter\n",
    "        filtered_data = sosfilt(sos, med_filt)\n",
    "        \n",
    "    return filtered_data\n",
    "\n",
    "def gravitational_filter(acc_data, fs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to filter out the gravitational component of ACC signals using a 3rd order butterworth lowpass filter with\n",
    "    a cuttoff frequency of 0.3 Hz\n",
    "    The implementation is based on:\n",
    "    \"A Public Domain Dataset for Human Activity Recognition Using Smartphones\"\n",
    "    https://www.esann.org/sites/default/files/proceedings/legacy/es2013-84.pdf\n",
    "    :param acc_data: a 1-D or (MxN) array, where where M is the signal length in samples and\n",
    "                 N is the number of signals / channels.\n",
    "    :param fs: the sampling frequency of the acc data.\n",
    "    :return: the gravitational component of each signal/channel contained in acc_data\n",
    "    \"\"\"\n",
    "    \n",
    "    #define the filter\n",
    "    order = 3\n",
    "    f_c = 0.5\n",
    "    sos = butter(order, f_c, fs=fs, output='sos')\n",
    "    \n",
    "    #copy the array\n",
    "    gravity_data = acc_data.copy()\n",
    "    \n",
    "    #check the dimensionality of the input\n",
    "    if gravity_data.ndim > 1:\n",
    "        #cycle of the channels contained in data\n",
    "        for channel in range(gravity_data.shape[1]):\n",
    "            #get the channel & apply butterworth filter\n",
    "            gravity_data[:, channel] = sosfilt(sos, acc_data[:, channel])\n",
    "    else:\n",
    "        gravity_data = sosfilt(sos, acc_data)\n",
    "        \n",
    "    return gravity_data\n",
    "\n",
    "def pre_process_inertial_data(sensor_data, is_acc=True, fs=40):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies the pre-processing pipeline of \"A Public Domain Dataset for Human Activity Recognition Using Smartphones\"\n",
    "    (https://www.esann.org/sites/default/files/proceedings/legacy/es2013-84.pdf). The pipeline consists of:\n",
    "    (1) applying a median filter\n",
    "    (2) applying a 3rd order low-pass filter with a cut-off at 18 Hz\n",
    "    (3) applying a 3rd order low-pass filter with a cut-off at 0.3 Hz to obtain gravitational component\n",
    "    (4) subtract gravitational component from ACC signal\n",
    "\n",
    "    :param sensor_data: the sensor data.\n",
    "    :param is_acc: boolean indicating whether the sensor is an accelerometer.\n",
    "    :param fs: the sampling frequency of the sensor data (in Hz).\n",
    "    :return: numpy.array containing the pre-processed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #apply median and lowpass filter\n",
    "    filtered_data = median_and_lowpass_filter(sensor_data, fs=fs)\n",
    "    #check if sensor is ACC\n",
    "    if is_acc:\n",
    "        #get the gravitacional component\n",
    "        gravitational_component = gravitational_filter(filtered_data, fs=fs)\n",
    "        #subtract the gravitacional component\n",
    "        filtered_data -= gravitational_component\n",
    "    return filtered_data\n",
    "\n",
    "def band_pass_filter(data, low_cutoff=0.1, high_cutoff=15, fs=40, order=3):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = low_cutoff / nyquist\n",
    "    high = high_cutoff / nyquist\n",
    "    b, a = butter(order, [low, high], btype=\"band\", analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def adaptive_baseline_correction(data, alpha=0.08):\n",
    "    baseline = np.zeros_like(data)\n",
    "    estimate = data[0]\n",
    "    for i in range(len(data)):\n",
    "        estimate = alpha * data[i] + (1 - alpha) * estimate\n",
    "        baseline[i] = data[i] - estimate\n",
    "    return baseline\n",
    "\n",
    "def savitzky_golay_smooth(data, window_size=5, poly_order=2):\n",
    "    return savgol_filter(data, window_size, poly_order)\n",
    "\n",
    "# === Process Each Sensor File ===\n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for file in converted_files:\n",
    "    sensor_name = file.replace(\"cleaned_data_converted_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"Processing accelerometer for sensor: {sensor_name}\")\n",
    "\n",
    "    # === Step 1: Remove Gravity ===\n",
    "    accel_data = df[[\"AccX\", \"AccY\", \"AccZ\"]].values\n",
    "    accel_no_gravity = pre_process_inertial_data(accel_data, is_acc=True, fs=fs)\n",
    "\n",
    "    # === Step 2: Band-pass Filter ===\n",
    "    accel_bandpassed = np.apply_along_axis(band_pass_filter, 0, accel_no_gravity)\n",
    "\n",
    "    # === Step 3: Baseline Correction ===\n",
    "    accel_corrected = np.apply_along_axis(adaptive_baseline_correction, 0, accel_bandpassed)\n",
    "\n",
    "    # === Step 4: Smoothing ===\n",
    "    accel_smooth = np.apply_along_axis(savitzky_golay_smooth, 0, accel_corrected)\n",
    "\n",
    "    # === Step 5: Save Processed Data ===\n",
    "    df[[\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\"]] = accel_smooth\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_accelerometer_data_{sensor_name}.csv\")\n",
    "    df.to_csv(processed_path, index=False)\n",
    "    print(f\" Saved processed accelerometer data: {processed_path}\")\n",
    "\n",
    "    # === Step 6: Plot for Visual Inspection ===\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    colors = {\"AccX\": \"blue\", \"AccY\": \"green\", \"AccZ\": \"red\"}\n",
    "\n",
    "    for i, axis in enumerate([\"AccX\", \"AccY\", \"AccZ\"]):\n",
    "        corrected_axis = axis + \"_processed\"\n",
    "        axes[i].plot(df.index, df[axis], label=f\"Raw {axis}\", color=colors[axis], alpha=0.5)\n",
    "        axes[i].plot(df.index, df[corrected_axis], label=f\"Processed {axis}\", linestyle=\"dashed\", color=\"black\")\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[i].set_xlabel(\"Time (samples)\")\n",
    "        axes[i].set_ylabel(f\"{axis} (m/sÂ²)\")\n",
    "        axes[i].set_title(f\"{sensor_name} - Raw vs Processed {axis}\")\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0e4e4",
   "metadata": {},
   "source": [
    "## Plot processed accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e89789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the directory containing processed accelerometer files\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "# Get all processed accelerometer files\n",
    "processed_files = [f for f in os.listdir(data_dir)\n",
    "                   if f.startswith(\"final_corrected_accelerometer_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Plot each file\n",
    "for file in processed_files:\n",
    "    sensor_name = file.replace(\"final_corrected_accelerometer_data_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "\n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Identify processed accelerometer columns\n",
    "    acc_columns = [col for col in df.columns if \"Acc\" in col and \"processed\" in col]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in acc_columns:\n",
    "        plt.plot(df[col], label=col, alpha=0.8)\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Accelerometer (m/sÂ²)\")\n",
    "    plt.title(f\"Processed Accelerometer Data â {sensor_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f94e3",
   "metadata": {},
   "source": [
    "# MAGNETOMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0da81b",
   "metadata": {},
   "source": [
    "## Recognizing which type of  drift we have in the magnetometer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Directory with processed files\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "converted_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "if not converted_files:\n",
    "    raise FileNotFoundError(\"No cleaned_data_converted_*.csv files found.\")\n",
    "\n",
    "for file in converted_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    sensor_name = file.split(\"converted_\")[1].replace(\".csv\", \"\")\n",
    "    \n",
    "    # Plot raw magnetometer data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"MagX\"], label=\"MagX\", alpha=0.8)\n",
    "    plt.plot(df[\"MagY\"], label=\"MagY\", alpha=0.8)\n",
    "    plt.plot(df[\"MagZ\"], label=\"MagZ\", alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Magnetometer (T)\")\n",
    "    plt.title(f\"Raw Magnetometer Data - Identifying Drift ({sensor_name})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Individual axis subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    axes[0].plot(df[\"MagX\"], label=\"MagX\", color=\"blue\", alpha=0.8)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[0].set_ylabel(\"Mag X (T)\")\n",
    "    axes[0].set_title(f\"{sensor_name} - MagX\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(df[\"MagY\"], label=\"MagY\", color=\"green\", alpha=0.8)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1].set_ylabel(\"Mag Y (T)\")\n",
    "    axes[1].set_title(f\"{sensor_name} - MagY\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(df[\"MagZ\"], label=\"MagZ\", color=\"red\", alpha=0.8)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"Mag Z (T)\")\n",
    "    axes[2].set_title(f\"{sensor_name} - MagZ\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7dd75",
   "metadata": {},
   "source": [
    "## Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806334f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, savgol_filter\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "# === Functions for Magnetometer Processing ===\n",
    "def hard_iron_correction(mag_data):\n",
    "    \"\"\"Apply hard-iron correction using median instead of mean to remove constant bias.\"\"\"\n",
    "    bias = np.median(mag_data, axis=0)\n",
    "    return mag_data - bias\n",
    "\n",
    "def ellipsoid_fit(mag_data):\n",
    "    \"\"\"Estimate soft-iron correction using ellipsoid fitting.\"\"\"\n",
    "    def cost_function(params, x, y, z):\n",
    "        A, B, C, D, E, F, G, H, I, J = params\n",
    "        return (A*x**2 + B*y**2 + C*z**2 + 2*D*x*y + 2*E*x*z + 2*F*y*z +\n",
    "                2*G*x + 2*H*y + 2*I*z + J)\n",
    "    \n",
    "    X, Y, Z = mag_data[:, 0], mag_data[:, 1], mag_data[:, 2]\n",
    "    initial_guess = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, -1])\n",
    "    result = least_squares(cost_function, initial_guess, args=(X, Y, Z))\n",
    "\n",
    "    # Extract transformation matrix parameters\n",
    "    A, B, C, D, E, F, G, H, I, J = result.x\n",
    "    M_inv_raw = np.array([[A, D, E], [D, B, F], [E, F, C]])\n",
    "    \n",
    "    # Normalize matrix using Singular Value Decomposition (SVD)\n",
    "    U, S, Vt = np.linalg.svd(M_inv_raw)\n",
    "    scale_factor = np.median(S)\n",
    "    M_inv = np.linalg.inv(M_inv_raw / scale_factor)\n",
    "    \n",
    "    # Extract offset (soft-iron bias)\n",
    "    soft_iron_bias = np.array([G, H, I])\n",
    "    return M_inv, soft_iron_bias\n",
    "\n",
    "def band_pass_filter(data, low_cutoff=0.5, high_cutoff=10, fs=40, order=2):\n",
    "    \"\"\"Apply a Butterworth band-pass filter to remove both low-frequency drift and high-frequency noise.\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = low_cutoff / nyquist\n",
    "    high = high_cutoff / nyquist\n",
    "    b, a = butter(order, [low, high], btype=\"band\", analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def savitzky_golay_smooth(data, window_size=9, poly_order=2):\n",
    "    \"\"\"Apply Savitzky-Golay smoothing to reduce noise while preserving trends.\"\"\"\n",
    "    return savgol_filter(data, window_size, poly_order)\n",
    "\n",
    "# === Process All Cleaned Files ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "cleaned_files = [f for f in os.listdir(data_dir) if f.startswith(\"cleaned_data_converted_\") and f.endswith(\".csv\")]\n",
    "\n",
    "if not cleaned_files:\n",
    "    raise FileNotFoundError(\"No cleaned_data_converted_<sensor>.csv files found.\")\n",
    "\n",
    "for file in cleaned_files:\n",
    "    sensor_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(sensor_path)\n",
    "    sensor_name = file.split(\"cleaned_data_converted_\")[1].replace(\".csv\", \"\")\n",
    "    df_raw = df.copy()  # For plotting later\n",
    "\n",
    "    # Extract and process magnetometer data\n",
    "    mag_columns = [\"MagX\", \"MagY\", \"MagZ\"]\n",
    "    mag_data = df[mag_columns].values\n",
    "\n",
    "    mag_hard_iron = hard_iron_correction(mag_data)\n",
    "    M_inv, soft_iron_bias = ellipsoid_fit(mag_hard_iron)\n",
    "    mag_soft_iron = np.dot(M_inv, (mag_hard_iron - soft_iron_bias).T).T\n",
    "    mag_filtered = np.apply_along_axis(band_pass_filter, 0, mag_soft_iron)\n",
    "    mag_smooth = np.apply_along_axis(savitzky_golay_smooth, 0, mag_filtered)\n",
    "\n",
    "    # Store in DataFrame\n",
    "    df[[\"MagX_processed\", \"MagY_processed\", \"MagZ_processed\"]] = mag_smooth\n",
    "\n",
    "    # Save file\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "    df.to_csv(processed_path, index=False)\n",
    "    print(f\"Saved magnetometer data for sensor: {sensor_name}\")\n",
    "\n",
    "# === Plot 4 Graphs Per Sensor ===\n",
    "for file in cleaned_files:\n",
    "    sensor_name = file.split(\"cleaned_data_converted_\")[1].replace(\".csv\", \"\")\n",
    "    raw_path = os.path.join(data_dir, f\"cleaned_data_converted_{sensor_name}.csv\")\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "    \n",
    "    if not os.path.exists(raw_path) or not os.path.exists(processed_path):\n",
    "        continue\n",
    "\n",
    "    df_raw = pd.read_csv(raw_path)\n",
    "    df_proc = pd.read_csv(processed_path)\n",
    "\n",
    "    # === Combined MagX, MagY, MagZ Raw vs Processed ===\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df_raw[\"MagX\"], label=\"Raw MagX\", color=\"blue\", alpha=0.3)\n",
    "    plt.plot(df_raw[\"MagY\"], label=\"Raw MagY\", color=\"green\", alpha=0.3)\n",
    "    plt.plot(df_raw[\"MagZ\"], label=\"Raw MagZ\", color=\"red\", alpha=0.3)\n",
    "\n",
    "    plt.plot(df_proc[\"MagX_processed\"], label=\"Processed MagX\", color=\"blue\", linestyle=\"--\")\n",
    "    plt.plot(df_proc[\"MagY_processed\"], label=\"Processed MagY\", color=\"green\", linestyle=\"--\")\n",
    "    plt.plot(df_proc[\"MagZ_processed\"], label=\"Processed MagZ\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.title(f\"{sensor_name} - Magnetometer Raw vs Processed (Combined)\")\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Magnetic Field (T)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Separate Axes Plots: Raw vs Processed ===\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "    colors = [\"blue\", \"green\", \"red\"]\n",
    "    mag_axes = [\"MagX\", \"MagY\", \"MagZ\"]\n",
    "\n",
    "    for i, axis in enumerate(mag_axes):\n",
    "        axes[i].plot(df_raw[axis], label=f\"Raw {axis}\", color=colors[i], alpha=0.3)\n",
    "        axes[i].plot(df_proc[f\"{axis}_processed\"], label=f\"Processed {axis}\", linestyle=\"--\", color=\"black\")\n",
    "        axes[i].set_title(f\"{sensor_name} - {axis} Raw vs Processed\")\n",
    "        axes[i].set_ylabel(\"Magnetic Field (T)\")\n",
    "        axes[i].set_xlabel(\"Time (samples)\")\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7332c",
   "metadata": {},
   "source": [
    "## Plot processed magnetometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352eeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Loop through each processed file ===\n",
    "for file in cleaned_files:\n",
    "    sensor_name = file.split(\"cleaned_data_converted_\")[1].replace(\".csv\", \"\")\n",
    "    processed_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(f\"Processed file not found for sensor: {sensor_name}\")\n",
    "        continue\n",
    "\n",
    "    # Load the processed DataFrame\n",
    "    df = pd.read_csv(processed_path)\n",
    "\n",
    "    # Print column names to verify structure (optional debug)\n",
    "    print(f\"\\nProcessed columns for {sensor_name}:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Identify processed magnetometer columns\n",
    "    mag_columns = [col for col in df.columns if \"Mag\" in col and \"processed\" in col]\n",
    "    print(f\"Processed magnetometer columns for {sensor_name}:\", mag_columns)\n",
    "\n",
    "    # Plot processed magnetometer data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in mag_columns:\n",
    "        plt.plot(df[col], label=col, alpha=0.8)\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Magnetometer (T)\")\n",
    "    plt.title(f\"{sensor_name} - Processed Magnetometer Data\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e14754",
   "metadata": {},
   "source": [
    "## Creating the combined final data file for the sensor fusion operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define data directory\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "# Get all processed accelerometer files\n",
    "accel_files = [f for f in os.listdir(data_dir) if f.startswith(\"final_corrected_accelerometer_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "for accel_file in accel_files:\n",
    "    # Extract sensor name\n",
    "    sensor_name = accel_file.replace(\"final_corrected_accelerometer_data_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    # Construct paths for all 3 processed files\n",
    "    accel_path = os.path.join(data_dir, f\"final_corrected_accelerometer_data_{sensor_name}.csv\")\n",
    "    gyro_path = os.path.join(data_dir, f\"final_corrected_gyroscope_data_{sensor_name}.csv\")\n",
    "    mag_path = os.path.join(data_dir, f\"final_corrected_magnetometer_data_{sensor_name}.csv\")\n",
    "\n",
    "    # Check if all 3 files exist\n",
    "    if not os.path.exists(accel_path) or not os.path.exists(gyro_path) or not os.path.exists(mag_path):\n",
    "        print(f\"Skipping {sensor_name}: One or more processed files are missing.\")\n",
    "        continue\n",
    "\n",
    "    # Load data\n",
    "    accel_df = pd.read_csv(accel_path)\n",
    "    gyro_df = pd.read_csv(gyro_path)\n",
    "    mag_df = pd.read_csv(mag_path)\n",
    "\n",
    "    # Required columns\n",
    "    accel_columns = [\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\"]\n",
    "    gyro_columns = [\"GyrX_kalman\", \"GyrY_kalman\", \"GyrZ_kalman\"]\n",
    "    mag_columns = [\"MagX_processed\", \"MagY_processed\", \"MagZ_processed\", \"PacketCounter\", \"Roll\", \"Pitch\", \"Yaw\"]\n",
    "\n",
    "    # Check column availability\n",
    "    for col in accel_columns:\n",
    "        if col not in accel_df.columns:\n",
    "            raise ValueError(f\"Missing column in {accel_file}: {col}\")\n",
    "    for col in gyro_columns:\n",
    "        if col not in gyro_df.columns:\n",
    "            raise ValueError(f\"Missing column in {gyro_file}: {col}\")\n",
    "    for col in mag_columns:\n",
    "        if col not in mag_df.columns:\n",
    "            raise ValueError(f\"Missing column in {mag_file}: {col}\")\n",
    "\n",
    "    # Concatenate the selected columns\n",
    "    combined_df = pd.concat([\n",
    "        accel_df[accel_columns].reset_index(drop=True),\n",
    "        gyro_df[gyro_columns].reset_index(drop=True),\n",
    "        mag_df[mag_columns].reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Save the combined dataset\n",
    "    combined_path = os.path.join(data_dir, f\"final_combined_sensor_data_{sensor_name}.csv\")\n",
    "    combined_df.to_csv(combined_path, index=False)\n",
    "    print(f\"Saved final combined data for sensor: {sensor_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaea6bd",
   "metadata": {},
   "source": [
    "## Estimating the Roll, Pitch and Yaw values based on the measured pre-processing sensor data through Sensor Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7636ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ahrs.filters import Mahony\n",
    "from ahrs.common.orientation import q2euler\n",
    "\n",
    "# === Settings ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "fs = 40  # Sampling rate (Hz)\n",
    "dt = 1 / fs\n",
    "\n",
    "# === Find All Sensor Files ===\n",
    "combined_files = [f for f in os.listdir(data_dir)\n",
    "                  if f.startswith(\"final_combined_sensor_data_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# === Process Each Sensor File ===\n",
    "for file in combined_files:\n",
    "    sensor_name = file.replace(\"final_combined_sensor_data_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"\\nProcessing {sensor_name} with Mahony filter...\")\n",
    "\n",
    "    # Required sensor columns\n",
    "    required_cols = [\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\",\n",
    "                     \"GyrX_kalman\", \"GyrY_kalman\", \"GyrZ_kalman\",\n",
    "                     \"MagX_processed\", \"MagY_processed\", \"MagZ_processed\"]\n",
    "    \n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Skipping {sensor_name}: Missing required columns.\")\n",
    "        continue\n",
    "\n",
    "    # === Extract Sensor Data ===\n",
    "    acc = df[[\"AccX_processed\", \"AccY_processed\", \"AccZ_processed\"]].values\n",
    "    gyr = df[[\"GyrX_kalman\", \"GyrY_kalman\", \"GyrZ_kalman\"]].values\n",
    "    mag = df[[\"MagX_processed\", \"MagY_processed\", \"MagZ_processed\"]].values\n",
    "\n",
    "    # === Initialize Mahony Filter ===\n",
    "    mahony = Mahony(sampleperiod=dt)\n",
    "    q = np.zeros((len(df), 4))\n",
    "    q[0] = np.array([1.0, 0.0, 0.0, 0.0])  # initial quaternion\n",
    "\n",
    "    # === Apply Mahony Filter ===\n",
    "    for i in range(1, len(df)):\n",
    "        q[i] = mahony.updateIMU(q[i-1], gyr=gyr[i], acc=acc[i])\n",
    "        # To include magnetometer: use mahony.update() if supported by the ahrs version\n",
    "        # q[i] = mahony.update(q[i-1], gyr=gyr[i], acc=acc[i], mag=mag[i])\n",
    "\n",
    "    # === Convert Quaternion to Euler Angles ===\n",
    "    euler_rad = np.array([q2euler(qi) for qi in q])\n",
    "    euler_deg = np.rad2deg(euler_rad)\n",
    "\n",
    "    # === Store Orientation Estimations ===\n",
    "    df[\"Roll_Estimated\"] = euler_deg[:, 0]\n",
    "    df[\"Pitch_Estimated\"] = euler_deg[:, 1]\n",
    "    df[\"Yaw_Estimated\"] = euler_deg[:, 2]\n",
    "\n",
    "    # === Save Output ===\n",
    "    output_path = os.path.join(data_dir, f\"final_orientation_estimation_{sensor_name}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved Mahony-estimated orientation for {sensor_name} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1f5a8",
   "metadata": {},
   "source": [
    "## Comparing the calculated orientation values in the previous step with the measured Roll, Pitch and Yaw by the IMUs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9149fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory with processed orientation files\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "\n",
    "# Find all orientation files\n",
    "orientation_files = [f for f in os.listdir(data_dir) if f.startswith(\"final_orientation_estimation_\") and f.endswith(\".csv\")]\n",
    "\n",
    "if not orientation_files:\n",
    "    raise FileNotFoundError(\"No orientation estimation files found.\")\n",
    "\n",
    "for file in orientation_files:\n",
    "    sensor_name = file.replace(\"final_orientation_estimation_\", \"\").replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"\\n Plotting Measured vs Estimated Orientation for: {sensor_name}\")\n",
    "\n",
    "    # === Plot Roll, Pitch, Yaw Comparison ===\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "    axes[0].plot(df[\"Roll\"], label=\"Measured Roll\", color='blue', alpha=0.6)\n",
    "    axes[0].plot(df[\"Roll_Estimated\"], label=\"Estimated Roll\", linestyle=\"dashed\", color='black')\n",
    "    axes[0].set_ylabel(\"Angle (Â°)\")\n",
    "    axes[0].set_title(f\"{sensor_name} - Roll: Measured vs Estimated\")\n",
    "    axes[0].legend()\n",
    "    axes[0].axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    axes[1].plot(df[\"Pitch\"], label=\"Measured Pitch\", color='green', alpha=0.6)\n",
    "    axes[1].plot(df[\"Pitch_Estimated\"], label=\"Estimated Pitch\", linestyle=\"dashed\", color='black')\n",
    "    axes[1].set_ylabel(\"Angle (Â°)\")\n",
    "    axes[1].set_title(f\"{sensor_name} - Pitch: Measured vs Estimated\")\n",
    "    axes[1].legend()\n",
    "    axes[1].axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    axes[2].plot(df[\"Yaw\"], label=\"Measured Yaw\", color='red', alpha=0.6)\n",
    "    axes[2].plot(df[\"Yaw_Estimated\"], label=\"Estimated Yaw\", linestyle=\"dashed\", color='black')\n",
    "    axes[2].set_xlabel(\"Time (samples)\")\n",
    "    axes[2].set_ylabel(\"Angle (Â°)\")\n",
    "    axes[2].set_title(f\"{sensor_name} - Yaw: Measured vs Estimated\")\n",
    "    axes[2].legend()\n",
    "    axes[2].axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1ad7d",
   "metadata": {},
   "source": [
    "## Convert Processed IMU Data to OpenSim Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200109e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Configuration ===\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "sampling_rate = 40  # Hz\n",
    "output_mot_path = os.path.join(data_dir, \"opensim_combined_motion.mot\")\n",
    "\n",
    "# === Sensor to OpenSim coordinate mapping ===\n",
    "sensor_to_opensim_mapping = {\n",
    "    \"chest\": {\n",
    "        \"thorax_tilt\": \"Pitch_Estimated\",\n",
    "        \"thorax_list\": \"Roll_Estimated\",\n",
    "        \"thorax_rotation\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"right-thigh\": {\n",
    "        \"hip_flexion_r\": \"Pitch_Estimated\",\n",
    "        \"hip_adduction_r\": \"Roll_Estimated\",\n",
    "        \"hip_rotation_r\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"right-knee\": {\n",
    "        \"knee_angle_r\": \"Pitch_Estimated\"\n",
    "    },\n",
    "    \"right-foot\": {\n",
    "        \"mtp_angle_r\": \"Pitch_Estimated\"\n",
    "    },\n",
    "    \"left-thigh\": {\n",
    "        \"hip_flexion_l\": \"Pitch_Estimated\",\n",
    "        \"hip_adduction_l\": \"Roll_Estimated\",\n",
    "        \"hip_rotation_l\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"left-knee\": {\n",
    "        \"knee_angle_l\": \"Pitch_Estimated\"\n",
    "    },\n",
    "    \"left-foot\": {\n",
    "        \"mtp_angle_l\": \"Pitch_Estimated\"\n",
    "    },\n",
    "    \"lumbar\": {\n",
    "        \"lumbar_extension\": \"Pitch_Estimated\",\n",
    "        \"lumbar_bending\": \"Roll_Estimated\",\n",
    "        \"lumbar_rotation\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"right-arm\": {\n",
    "        \"arm_flex_r\": \"Pitch_Estimated\",\n",
    "        \"arm_add_r\": \"Roll_Estimated\",\n",
    "        \"arm_rot_r\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"right-wrist\": {\n",
    "        \"wrist_flex_r\": \"Pitch_Estimated\",\n",
    "        \"wrist_dev_r\": \"Roll_Estimated\"\n",
    "    },\n",
    "    \"right-hand\": {\n",
    "        \"pro_sup_r\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"left-arm\": {\n",
    "        \"arm_flex_l\": \"Pitch_Estimated\",\n",
    "        \"arm_add_l\": \"Roll_Estimated\",\n",
    "        \"arm_rot_l\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"left-wrist\": {\n",
    "        \"wrist_flex_l\": \"Pitch_Estimated\",\n",
    "        \"wrist_dev_l\": \"Roll_Estimated\"\n",
    "    },\n",
    "    \"left-hand\": {\n",
    "        \"pro_sup_l\": \"Yaw_Estimated\"\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"head_flexion\": \"Pitch_Estimated\",\n",
    "        \"head_rotation\": \"Yaw_Estimated\",\n",
    "        \"head_bending\": \"Roll_Estimated\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Load and combine data ===\n",
    "combined_df = None\n",
    "time_column = None\n",
    "used_columns = [\"time\"]  # Start with time\n",
    "\n",
    "for sensor_name, joint_mapping in sensor_to_opensim_mapping.items():\n",
    "    file_path = os.path.join(data_dir, f\"final_orientation_estimation_{sensor_name}.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: File not found for sensor '{sensor_name}' at {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "# Ensure time column\n",
    "    if \"PacketCounter\" in df.columns:\n",
    "        df[\"time\"] = df[\"PacketCounter\"] / sampling_rate\n",
    "        df[\"time\"] = df[\"time\"] - df[\"time\"].iloc[0]\n",
    "    elif \"time\" not in df.columns:\n",
    "        dt = 1 / sampling_rate\n",
    "        df[\"time\"] = np.arange(0, len(df)*dt, dt)\n",
    "        df[\"time\"] = df[\"time\"] - df[\"time\"].iloc[0]\n",
    "\n",
    "    if time_column is None:\n",
    "        time_column = df[\"time\"].values\n",
    "        combined_df = pd.DataFrame({\"time\": time_column})\n",
    "\n",
    "    for joint_name, source_column in joint_mapping.items():\n",
    "        if source_column not in df.columns:\n",
    "            print(f\"Warning: Missing {source_column} in {sensor_name}. Skipping {joint_name}.\")\n",
    "            continue\n",
    "        combined_df[joint_name] = df[source_column].values\n",
    "        used_columns.append(joint_name)\n",
    " \n",
    "# === Create OpenSim Header ===\n",
    "header = [\n",
    "    \"name Combined_Motion\",\n",
    "    f\"datacolumns {len(used_columns)}\",\n",
    "    f\"datarows {len(combined_df)}\",\n",
    "    f\"range {combined_df['time'].iloc[0]} {combined_df['time'].iloc[-1]}\",\n",
    "    \"endheader\"\n",
    "]\n",
    "\n",
    "# === Save to .mot ===\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(header) + \"\\n\")\n",
    "    f.write(\"\\t\".join(used_columns) + \"\\n\")\n",
    "    combined_df.to_csv(f, sep=\"\\t\", columns=used_columns, index=False, header=False, float_format=\"%.6f\")\n",
    "\n",
    "print(f\" Combined OpenSim .mot file created at:\\n{output_mot_path}\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9273f84",
   "metadata": {},
   "source": [
    "## Creating the orientation files for OpenSim Inverse Kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Configuration\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "sampling_rate = 40  # Hz\n",
    "input_pattern = os.path.join(data_dir, \"final_orientation_estimation_*.csv\")\n",
    "\n",
    "# Find all relevant CSV files\n",
    "files = glob.glob(input_pattern)\n",
    "\n",
    "for file_path in files:\n",
    "    try:\n",
    "        # Extract body segment name from filename\n",
    "        base_name = os.path.basename(file_path)\n",
    "        segment = base_name.replace(\"final_orientation_estimation_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "        # Load required columns only\n",
    "        use_columns = [\"PacketCounter\", \"Roll_Estimated\", \"Pitch_Estimated\", \"Yaw_Estimated\"]\n",
    "        df = pd.read_csv(file_path, usecols=lambda c: c in use_columns or c == \"time\")\n",
    "\n",
    "        # Ensure time column\n",
    "        if \"PacketCounter\" in df.columns:\n",
    "            df[\"time\"] = df[\"PacketCounter\"] / sampling_rate\n",
    "            df[\"time\"] = df[\"time\"] - df[\"time\"].iloc[0]\n",
    "        elif \"time\" not in df.columns:\n",
    "            dt = 1 / sampling_rate\n",
    "            df[\"time\"] = np.arange(0, len(df) * dt, dt)\n",
    "            df[\"time\"] = df[\"time\"] - df[\"time\"].iloc[0]\n",
    "\n",
    "        # Select and rename columns\n",
    "        sto_df = df[[\"time\", \"Roll_Estimated\", \"Pitch_Estimated\", \"Yaw_Estimated\"]].copy()\n",
    "        sto_df.columns = [\n",
    "            \"time\",\n",
    "            f\"{segment}_roll\",\n",
    "            f\"{segment}_pitch\",\n",
    "            f\"{segment}_yaw\"\n",
    "        ]\n",
    "\n",
    "        # Save to .sto \n",
    "        sto_filename = f\"{segment}_orientation.sto\"\n",
    "        sto_path = os.path.join(data_dir, sto_filename)\n",
    "        sto_df.to_csv(sto_path, sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "        print(f\"STO created: {sto_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec78f5",
   "metadata": {},
   "source": [
    "## Creating the orientation file at placement pose for IMU placement in OpenSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "data_dir = \"/Users/afonsoamaral/Desktop/data\"\n",
    "input_pattern = os.path.join(data_dir, \"*_orientation.sto\")\n",
    "output_file = os.path.join(data_dir, \"full_placement_orientation.sto\")\n",
    "\n",
    "# Collect one-row DataFrames from each file\n",
    "placement_rows = []\n",
    "\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Take only first row\n",
    "        row = df.iloc[[0]].copy()\n",
    "\n",
    "        # Ensure column names are already in OpenSim format\n",
    "        placement_rows.append(row)\n",
    "        print(f\" Added: {os.path.basename(file_path)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to process {file_path}: {e}\")\n",
    "\n",
    "# Merge all into single-row DataFrame\n",
    "if placement_rows:\n",
    "    full_df = placement_rows[0]\n",
    "    for next_df in placement_rows[1:]:\n",
    "        full_df = pd.merge(full_df, next_df.drop(columns=[\"time\"]), left_index=True, right_index=True)\n",
    "\n",
    "    # Save to .sto file\n",
    "    full_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"\\n Placement orientation file created:\\n{output_file}\")\n",
    "else:\n",
    "    print(\" No placement data assembled.\")\n",
    "\n",
    "df_preview.iloc[:, :10]  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492eeee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd9ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18061b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00221b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf1b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df08da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ee6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b558e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b4c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d07719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2e772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbcc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c199740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597932e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
